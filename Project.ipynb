{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning project - July 2024\n",
    "### Luca Bertone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the goal has been to implement a Reinforcement Learning algorithm to learn how to play Quentin, a board game.\n",
    "\n",
    "The game consists of a $13x13$ grid, according to the rules of the game, and two players have to place some *stones* on the intersections of this grid, aiming at completing a way of orthogonally adjacent stones from one side of the grid to the other. Specifically, the **Black** player, has to complete a way such that it connects the top to the bottom of the grid, while the **White** has to complete a left-to-right way.\n",
    "\n",
    "The implementation of the game (Quentin.py) and the detailed rules (Quentin.pdf) are attached to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Quentin import QuentinGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPU if available\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Reinforcement Learning, one could try to describe this game in terms of MDP components, where:\n",
    "\n",
    "‚Ä¢ $S$ is the space of states: one state corresponds to a specific pattern of $\\{-1,0,1\\}$ values, where an empty intersection has a $-1$ value, and a filled intersection has a value in $\\{0,1\\}$, for the presence of a black stone or a white stone, respectively. <br>Given a $M\\times M$-dimensional grid, one could assume that $|S|=3^{M\\times M}$. Of course a lot of combinations should be discarded since they are not compliant with the game rules, but still the state space is huge.\n",
    "\n",
    "‚Ä¢ $A$ is the space of actions: in general, an upper bound for the number of actions available at any state is $|A|=M\\times M$, but the number of actions decreases at any time step and, again, the rules are such that some actions might be discarded a priori for a certain state configuration.\n",
    "\n",
    "‚Ä¢ $R$ is the reward structure, for which there are not guidelines: it has to be established during the implementation of the RL algorithm.\n",
    "\n",
    "‚Ä¢ $p(s',r|s,a)$, the transition probabilities are not known. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main issue of this application is the dimensionality of the state space, which makes it not possible to treat the problem in a tabular form.<br>\n",
    "For this reason, the approach which will be presented in what follows is **Deep Q-Learning** (DQN), a reinforcement learning algorithm that combines Q-learning with deep neural networks. Specifically, DQN employs a deep neural network to approximate the Q-function, allowing it to handle environments with large or continuous state spaces.<br>\n",
    "The Q-function, also known as the action-value function, is a function that predicts the expected cumulative future rewards that an agent will receive, starting from a given state and taking a specific action: $Q(s,a)= \\mathop{{}\\mathbb{E}}[r_t + \\gamma * max_{a'}Q(s_{t+1},a')|s_t=s, a_t=a]$. <br>\n",
    "The network is trained to minimize the difference between its Q-value predictions and the actual rewards received."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Local image](./Pseudo-code-of-deep-Q-learning-with-experience-replay.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The building blocks for this algorithm are:<br>\n",
    "(a) a neural network architecture,<br>\n",
    "(b) a memory object to store the past transitions: $(s_t,a_t,r_t,s_{t+1})$<br> \n",
    "(b) a mechanism for minibatch sampling from the transitions, needed during the training phase (by randomly sampling from the experiences, this breaks the bias that may have come from the sequential nature of a particular environment),<br>\n",
    "(c) auxiliary functions to perform all the intermediate operations: store transitions into memory, take next action, update parameters, convert the state data in a suitable format to be the input of the neural network, rank the actions according to their values.<br><br>\n",
    "\n",
    "Moreover, in many implementations two distinct neural networks are introduced. These networks have the same architecture but different weights. Every $N$ steps, the weights from the **main network** are copied to the **target network**. Using both of these networks leads to more stability in the learning process and helps the algorithm to learn more effectively. <br><br>\n",
    "Most of this will be part of a newly defined class: the **Agent**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network class\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, kernel_dim=128):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(state_size, kernel_dim)\n",
    "        self.layer2 = torch.nn.Linear(kernel_dim, kernel_dim)\n",
    "        self.layer3 = torch.nn.Linear(kernel_dim, action_size)        \n",
    "        self._initialize_weights()      # Apply He initialization to the layers\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Kaiming initialization: a good initialization strategy for NN with Relu activations\n",
    "        (https://towardsdatascience.com/all-ways-to-initialize-your-neural-network-16a585574b52)\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.layer1(x))\n",
    "        x = torch.nn.functional.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ChatGPT contribution to define the general structure of the class and the functions to be implemented##\n",
    "\n",
    "class QuentinDQNAgent:\n",
    "    \"\"\"\n",
    "    Agent class\n",
    "    \"\"\"\n",
    "    def __init__(self, size, state_size, action_size, **kwargs):\n",
    "        self.size = size\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        ## Parameters\n",
    "        self.gamma = kwargs.get('gamma', 0.95)\n",
    "        self.epsilon = kwargs.get('epsilon', 1.0)\n",
    "        self.epsilon_max = kwargs.get('epsilon_max', 1.0)\n",
    "        self.epsilon_min = kwargs.get('epsilon_min', 0.01)\n",
    "        self.epsilon_decay = kwargs.get('epsilon_decay', 0.9)\n",
    "        self.learning_rate = kwargs.get('learning_rate', 0.01)\n",
    "        self.tau = kwargs.get('tau', 0.01)\n",
    "        ## Neural networks: Policy network and Target network\n",
    "        self.model = DQN(state_size, action_size, kernel_dim=kwargs.get('kernel_size', 128)).to(device)\n",
    "        self.target_model = DQN(state_size, action_size, kernel_dim=kwargs.get('kernel_size', 128)).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1000, gamma=0.95)\n",
    "        ## Loss function: \n",
    "        #  The Huber loss acts like the mean squared error when the error is small, but like the mean absolute error when\n",
    "        #  the error is large - this makes it more robust to outliers when the estimates of ùëÑ are very noisy\n",
    "        #  (https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    "        self.criterion = torch.nn.SmoothL1Loss()\n",
    "        # self.criterion = torch.nn.functional.MSELoss()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        define how to update the Target network's weights\n",
    "        \"\"\"\n",
    "        # \"Hard\" update \"once in a while\"\n",
    "        # self.target_model.load_state_dict(self.model.state_dict())\n",
    "        \n",
    "        ## Soft update controlled by the hyperparameter TAU: the target network is updated at every step \n",
    "        target_model_state_dict = self.target_model.state_dict()\n",
    "        model_state_dict = self.model.state_dict()\n",
    "        for key in model_state_dict:\n",
    "            target_model_state_dict[key] = model_state_dict[key]*self.tau + target_model_state_dict[key]*(1-self.tau)\n",
    "        self.target_model.load_state_dict(target_model_state_dict)\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        add information from each step to the memory of the model: \n",
    "        this information will be crucial for the \"replay\" step, since the \n",
    "        minibatches come from here\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def act(self, state, unavail):\n",
    "        \"\"\"\n",
    "        the agent selects an action based on the current state or at random\n",
    "        \"\"\"\n",
    "        available_actions = list(set(range(self.action_size)) - set(unavail))\n",
    "        if not available_actions:\n",
    "            return None\n",
    "        ## if the random value is smaller than epsilon, extract a random action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.choice(available_actions)\n",
    "        #  otherwise, extract the action with the best value estimate\n",
    "        act_values = self.model(torch.FloatTensor(state))\n",
    "        available_q_values = act_values[available_actions]\n",
    "        return available_actions[torch.argmax(available_q_values).item()]\n",
    "\n",
    "\n",
    "    def epsilon_update(self, episode):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            return self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-self.epsilon_decay * episode)\n",
    "        else:\n",
    "            return self.epsilon\n",
    "\n",
    "\n",
    "    def replay(self, batch_size, losses=[]):\n",
    "        \"\"\"\n",
    "        the core of the learning procedure: optimization of the weights of the Policy network, based on the \n",
    "        loss, which is calculated for the tuples in the minibatch (a random sample of the memory of the model)\n",
    "        \"\"\"\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # TD target (Q-Target) using Target network\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * torch.max(self.target_model(torch.FloatTensor(next_state))).item()\n",
    "            # Former Q-value estimation (using Policy network)\n",
    "            target_f = self.model(torch.FloatTensor(state))\n",
    "            target_f[action] = target\n",
    "            # Loss value and optimization step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(target_f, self.model(torch.FloatTensor(state)))\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "        # if self.epsilon > self.epsilon_min:\n",
    "        #     self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.model.state_dict(), name)\n",
    "\n",
    "\n",
    "def load_model(model_path, state_size, action_size, kernel_size=128):\n",
    "    model = DQN(state_size, action_size, kernel_size)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "\n",
    "def convert_state(board, size, player):\n",
    "    \"\"\"\n",
    "    convert the relevant information for the state of the agent (the board matrix & the binary for the player) \n",
    "    into an array of length: size*size + 1\n",
    "    \"\"\"\n",
    "    state = np.zeros((size * size + 1))\n",
    "    for i, val in enumerate(board):\n",
    "        state[i] = val\n",
    "    state[-1] = player\n",
    "    return state\n",
    "\n",
    "\n",
    "def rank_actions(model, state):\n",
    "    \"\"\"\n",
    "    when applied to a certain input (environment state), the network gives as output the q-values \n",
    "    associated to the various actions, in the original order used for the training.\n",
    "    this utility function ranks the actions based on their q-value in decreasing order. \n",
    "    \"\"\"\n",
    "    state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state_tensor).numpy().flatten()\n",
    "    ranked_actions = np.argsort(q_values)[::-1]  # Sort actions by Q-value in descending order\n",
    "    q_values = q_values[ranked_actions]\n",
    "    return ranked_actions, q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several options for training a RL agent in the context of competitive games.<br>\n",
    "For example, one could try to train the two agents simultaneously, so that each agent continuously adapts to the strategies of the other. <br>\n",
    "In what follows, an implementation of such a strategy is presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simultaneous_rewards(gameover):\n",
    "    \"\"\"\n",
    "    the rewards structure of this problem:\n",
    "    ‚Ä¢ an agent receives no reward if, after their action, the opponent took a move which didn't make the game finish,\n",
    "    ‚Ä¢ an agent receives a very negative reward (-100) and a very positive reward (+100) if he loses or he wins the game, respectively.\n",
    "    \"\"\"\n",
    "    if gameover == -1:\n",
    "        black_reward = 0\n",
    "        white_reward = 0\n",
    "        done = False\n",
    "    elif gameover == 0:\n",
    "        black_reward = 100 \n",
    "        white_reward = -100\n",
    "        done = True\n",
    "    else:\n",
    "        black_reward = -100 \n",
    "        white_reward = 100\n",
    "        done = True    \n",
    "    return black_reward, white_reward, done\n",
    "\n",
    "\n",
    "def simultaneous_learning(size=7, episodes=20, batch_size=128, epsilon_decay=0.95, learning_rate=0.01, save_episodes=None):\n",
    "    \"\"\"\n",
    "    run the algorithm:\n",
    "    first, initialize the environment and some important variables; \n",
    "    then, start to alternatively pick an action with the black and with the white player;\n",
    "    each of them uses the opponent's move to evaluate their own actions.\n",
    "    \n",
    "    Hyperparameters:\n",
    "    self.gamma = kwargs.get('gamma', 0.95)\n",
    "    self.epsilon = kwargs.get('epsilon', 1.0)\n",
    "    self.epsilon_max = kwargs.get('epsilon_max', 1.0)\n",
    "    self.epsilon_min = kwargs.get('epsilon_min', 0.01)\n",
    "    self.epsilon_decay = kwargs.get('epsilon_decay', 0.9)\n",
    "    self.learning_rate = kwargs.get('learning_rate', 0.01)\n",
    "    self.tau = kwargs.get('tau', 0.01)\n",
    "    \"\"\"\n",
    "    state_size = size * size + 1    # size*size is the dimension of the board, the additional unit denotes the player\n",
    "    action_size = size * size    #potentially, the available actions are represented by the board \n",
    "    \n",
    "    # Initialize two agents which will learn by playing against each other\n",
    "    black_agent = QuentinDQNAgent(size, state_size, action_size, kernel_size=128)\n",
    "    white_agent = QuentinDQNAgent(size, state_size, action_size, kernel_size=128)\n",
    "\n",
    "    steps = 0\n",
    "    losses_black = []\n",
    "    losses_white = []\n",
    "\n",
    "    for e in (range(episodes+1)):\n",
    "        # Start a new game\n",
    "        # print(\"Episode: \" + str(e))\n",
    "        game = QuentinGame(size)\n",
    "        done = False\n",
    "        first_action = True\n",
    "        not_available_actions_black = []      \n",
    "        not_available_actions_white = []     # it holds the indexes of the actions which cannot be picked by the player in a certain state;\n",
    "                                            # after the player tries to take a \"forbidden\" action, it is inserted here \n",
    "                                            # and the player has to choose another action from a subset of the original action space \n",
    "        \n",
    "        while not done:            \n",
    "            ## BLACK's turn\n",
    "            black_state = convert_state(game.board, size, 0)\n",
    "            black_action = black_agent.act(black_state, [])\n",
    "            \n",
    "            # if the black's move is illegal, look for another action\n",
    "            while black_action is not None and not game.update_board(True, black_action, not_available_actions_black):\n",
    "                black_reward = -1\n",
    "                # store in memory the \"transition\" (although no transition actually happened):\n",
    "                black_next_state = convert_state(game.board, size, 0)\n",
    "                black_agent.remember(black_state, black_action, black_reward, black_next_state, done)\n",
    "                black_state = black_next_state\n",
    "                # look for a new action:\n",
    "                black_action = black_agent.act(black_state, not_available_actions_black)    \n",
    "\n",
    "                # if no moves are available for the black, it's a Draw\n",
    "                # assign rewards: for example, a negative one for the black (being the first to play, should try to win), and zero for the white \n",
    "                if black_action is None:\n",
    "                    black_reward = -10\n",
    "                    white_reward = 0\n",
    "                    done = True\n",
    "                    break\n",
    "            #  and start a new episode\n",
    "            if done:\n",
    "              break\n",
    "\n",
    "            # if the black player picked an action, assign rewards to both players\n",
    "            if black_action is not None:           \n",
    "                black_reward, white_reward, done = simultaneous_rewards(game.gameover())\n",
    "\n",
    "            ###the white agent waits until the black does a new move to store info on the next state\n",
    "            ###this condition prevents from storing a \"new\" state for the white agent after the very first move of the black\n",
    "            if first_action is not True:\n",
    "                if white_action is not None:\n",
    "                    white_next_state = convert_state(game.board, size, 1)\n",
    "                    white_agent.remember(white_state, white_action, white_reward, white_next_state, done)\n",
    "            else:\n",
    "                first_action = False\n",
    "\n",
    "            ## WHITE's turn\n",
    "            if not done:\n",
    "                white_state = convert_state(game.board, size, 1)\n",
    "                white_action = white_agent.act(white_state, [])                \n",
    "                # if the white's move is illegal, look for another action\n",
    "                while white_action is not None and not game.update_board(False, white_action, not_available_actions_white):\n",
    "                    white_reward = -1\n",
    "                    white_next_state = convert_state(game.board, size, 1)\n",
    "                    white_agent.remember(white_state, white_action, white_reward, white_next_state, done)\n",
    "                    white_state = white_next_state\n",
    "                    white_action = white_agent.act(white_state, not_available_actions_white)\n",
    "                    \n",
    "\n",
    "                    # if no moves are available for the white, it's a Draw\n",
    "                    # assign rewards: for example, a negative one for the black (being the first to play, should try to win), and zero for the white \n",
    "                    if white_action is None:\n",
    "                        black_reward = -10\n",
    "                        white_reward = 0\n",
    "                        done = True\n",
    "                        break\n",
    "                #  and start a new episode\n",
    "                if done:\n",
    "                    break\n",
    "              \n",
    "                # if the white player picked an action, assign rewards to both players\n",
    "                if white_action is not None:\n",
    "                      black_reward, white_reward, done = simultaneous_rewards(game.gameover())\n",
    "\n",
    "            # the black agent waits until the white does a new move to store info on the next state\n",
    "            if black_action is not None:\n",
    "                black_next_state = convert_state(game.board, size, 0)\n",
    "                black_agent.remember(black_state, black_action, black_reward, black_next_state, done)\n",
    "\n",
    "            # store next state for the white agent, in the event it wins over the black agent\n",
    "            # without this (duplicated) code the white would not be able to save info after a victory (done==True, out of the while loop)  \n",
    "            if done:\n",
    "                white_next_state = convert_state(game.board, size, 1)\n",
    "                white_agent.remember(white_state, white_action, white_reward, white_next_state, done)\n",
    "            \n",
    "            # Training phase (minibatch sampling and policy parameters optimization)\n",
    "            steps = steps + 1\n",
    "            if steps % 4 == 0:\n",
    "                if len(black_agent.memory) > batch_size and len(white_agent.memory) > batch_size:\n",
    "                  black_agent.epsilon = black_agent.epsilon_update(e)\n",
    "                  white_agent.epsilon = white_agent.epsilon_update(e)\n",
    "                  black_agent.replay(batch_size, losses_black)\n",
    "                  white_agent.replay(batch_size, losses_white)\n",
    "\n",
    "            # Target network update\n",
    "            if steps % 100 == 0:\n",
    "                black_agent.update_target_model()\n",
    "                white_agent.update_target_model()\n",
    "        \n",
    "        if e in save_episodes:\n",
    "            black_agent.save(f\"quentin_sz{size}_ep{e}__black\")\n",
    "            white_agent.save(f\"quentin_sz{size}_ep{e}__white\")\n",
    "\n",
    "    return losses_black, losses_white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative approach is to **alternate training sessions** where one agent learns while the other remains fixed. Again, the implementation is below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternate_rewards(gameover, is_black):\n",
    "    \"\"\"\n",
    "    the rewards structure of this problem:\n",
    "    ‚Ä¢ an agent receives no reward if, after their action, the opponent took a move which didn't make the game finish,\n",
    "    ‚Ä¢ an agent receives a very negative reward (-100) and a very positive reward (+100) if he loses or he wins the game, respectively.\n",
    "    \"\"\"\n",
    "    if gameover == -1:\n",
    "        reward = 0\n",
    "        done = False\n",
    "    elif gameover == 0:\n",
    "        reward = 100 if is_black else -100\n",
    "        done = True\n",
    "    else:\n",
    "        reward = -100 if is_black else 100\n",
    "        done = True    \n",
    "    return reward, done\n",
    "\n",
    "\n",
    "def next_move(board, size, agent, is_black, illegal):        \n",
    "    \"\"\"\n",
    "    Auxiliary function for the selection of the optimal action, given a fixed policy\n",
    "    \"\"\"\n",
    "    filled_locations = [i for i, x in enumerate(board) if x != -1]\n",
    "    valid_move = False\n",
    "    tmp = None\n",
    "\n",
    "    state = convert_state(board, size, not is_black)\n",
    "    ranked_actions, q_values = rank_actions(agent, state)\n",
    "    i = 0\n",
    "    while not valid_move and i < len(ranked_actions):\n",
    "        next_best = ranked_actions[i]\n",
    "        valid_move = next_best not in list(set(filled_locations) | set(illegal)) and 0 <= next_best < len(board)\n",
    "        if valid_move:\n",
    "            return next_best\n",
    "        i += 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternate_learning(learner_is_white, agent_name, size=7, episodes=20, batch_size=128, save_episodes=None):\n",
    "\n",
    "    if learner_is_white:\n",
    "        return white_learning(agent_name, size, episodes, batch_size, save_episodes)\n",
    "    else:\n",
    "        return black_learning(agent_name, size, episodes, batch_size, save_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_learning(agent_name, size, episodes, batch_size, epsilon_decay, learning_rate, tau, save_episodes):\n",
    "\n",
    "    state_size = size * size + 1    # size*size is the dimension of the board, the additional unit denotes the player\n",
    "    action_size = size * size    #potentially, the available actions are represented by the board\n",
    "    losses = []\n",
    "\n",
    "    agent_black = load_model(agent_name, state_size, action_size)\n",
    "    agent_white = QuentinDQNAgent(size, state_size, action_size)\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    for e in range(episodes + 1):\n",
    "        # print(\"Episode \" + str(e))\n",
    "        game = QuentinGame(size)\n",
    "        done = False\n",
    "        first_action = True\n",
    "        not_available_actions_black = [] \n",
    "        not_available_actions_white = []      \n",
    "\n",
    "        while not done:            \n",
    "            steps = steps + 1\n",
    "            ## BLACK's turn (optimal action according to the policy of agent_white)\n",
    "            black_action = next_move(game.board, size, agent_black, True, not_available_actions_black)\n",
    "            while black_action is not None and not game.update_board(True, black_action, not_available_actions_black):\n",
    "                black_action = next_move(game.board, size, agent_black, True, not_available_actions_black)\n",
    "\n",
    "            if black_action is None:\n",
    "                white_reward = 0\n",
    "                done = True\n",
    "                break\n",
    "            else:\n",
    "                white_reward, done = alternate_rewards(game.gameover(), False)\n",
    "\n",
    "            if not first_action:\n",
    "                if white_action is not None:\n",
    "                    white_next_state = convert_state(game.board, size, 1)\n",
    "                    agent_white.remember(white_state, white_action, white_reward, white_next_state, done)\n",
    "            else:\n",
    "                first_action = False\n",
    "\n",
    "            if done:\n",
    "                white_next_state = convert_state(game.board, size, 1)\n",
    "                agent_white.remember(white_state, white_action, white_reward, white_next_state, done)\n",
    "                break\n",
    "\n",
    "            ## WHITE's turn\n",
    "            white_state = convert_state(game.board, size, 1)\n",
    "            white_action = agent_white.act(white_state, not_available_actions_white)\n",
    "            while white_action is not None and not game.update_board(False, white_action, not_available_actions_white):\n",
    "                white_reward = -1\n",
    "                white_next_state = convert_state(game.board, size, 1)\n",
    "                agent_white.remember(white_state, white_action, white_reward, white_next_state, done)\n",
    "                white_state = white_next_state\n",
    "                white_action = agent_white.act(white_state, not_available_actions_white)\n",
    "\n",
    "            if white_action is None:\n",
    "                white_reward = 0\n",
    "                done = True\n",
    "                break\n",
    "                \n",
    "            if done:\n",
    "                white_next_state = convert_state(game.board, size, 1)\n",
    "                agent_white.remember(white_state, white_action, white_reward, white_next_state, done)\n",
    "                break\n",
    "\n",
    "            if steps % 15 == 0:\n",
    "                if len(agent_white.memory) > batch_size:\n",
    "                    agent_white.epsilon = agent_white.epsilon_update(e)\n",
    "                    agent_white.replay(batch_size, losses)\n",
    "\n",
    "            # if steps % 100 == 0:\n",
    "            agent_white.update_target_model()\n",
    "        \n",
    "        if e in save_episodes:\n",
    "            agent_black.save(f\"quentin_sz{size}_ep{e}__white_alternating\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_learning(agent_name, size, episodes, batch_size, save_episodes):\n",
    "\n",
    "    state_size = size * size + 1    # size*size is the dimension of the board, the additional unit denotes the player\n",
    "    action_size = size * size    #potentially, the available actions are represented by the board\n",
    "    losses = []\n",
    "\n",
    "    agent_black = QuentinDQNAgent(size, state_size, action_size)\n",
    "    agent_white = load_model(agent_name, state_size, action_size)\n",
    "\n",
    "    steps = 0\n",
    "\n",
    "    for e in range(episodes + 1):\n",
    "        # print(\"Episode \" + str(e))\n",
    "        game = QuentinGame(size)\n",
    "        done = False\n",
    "        first_action = True\n",
    "        not_available_actions_black = [] \n",
    "        not_available_actions_white = []      \n",
    "\n",
    "        while not done:            \n",
    "            steps = steps + 1\n",
    "            ## BLACK's turn\n",
    "            black_state = convert_state(game.board, size, 0)\n",
    "            black_action = agent_black.act(black_state, not_available_actions_black)\n",
    "            \n",
    "            while black_action is not None and not game.update_board(True, black_action, not_available_actions_black):\n",
    "                black_reward = -1\n",
    "                black_next_state = convert_state(game.board, size, 0)\n",
    "                agent_black.remember(black_state, black_action, black_reward, black_next_state, done)\n",
    "                black_state = black_next_state\n",
    "                black_action = agent_black.act(black_state, not_available_actions_black)\n",
    "\n",
    "            if black_action is None:\n",
    "                black_reward = 0\n",
    "                done = True\n",
    "                break\n",
    "            \n",
    "            ## WHITE's turn (optimal action according to the policy of agent_white)\n",
    "            white_action = next_move(game.board, size, agent_white, False, not_available_actions_white)\n",
    "            while white_action is not None and not game.update_board(False, white_action, not_available_actions_white):\n",
    "                white_action = next_move(game.board, size, agent_white, False, not_available_actions_white)\n",
    "\n",
    "            if white_action is None:\n",
    "                black_reward = 0\n",
    "                done = True\n",
    "                break\n",
    "            else:\n",
    "                black_reward, done = alternate_rewards(game.gameover(), True)\n",
    "\n",
    "            if done:\n",
    "                black_next_state = convert_state(game.board, size, 0)\n",
    "                agent_black.remember(black_state, black_action, black_reward, black_next_state, done)\n",
    "                break\n",
    "\n",
    "            if steps % 15 == 0:\n",
    "                if len(agent_black.memory) > batch_size:\n",
    "                    agent_black.epsilon = agent_black.epsilon_update(e)\n",
    "                    agent_black.replay(batch_size, losses)\n",
    "\n",
    "            agent_black.update_target_model()\n",
    "        \n",
    "        if e in save_episodes:\n",
    "                agent_black.save(f\"quentin_sz{size}_ep{e}__black_alternating\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's try to use a **hybrid approach**, starting with simultaneuous learning so as to have a baseline policy for each player, then employing alternate learning for one agent at a time, with the other keeping its policy unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_x_simult, L_y_simult = simultaneous_learning(size=7, episodes=25, batch_size=30, epsilon_decay=0.95, learning_rate=0.02, save_episodes=[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can keep one of the two policies fixed and try to improve the other one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_agent = \"quentin_sz7_ep25__white\"\n",
    "losses_black = alternate_learning(False, white_agent, size=7, episodes=25, batch_size=30, save_episodes=[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15745e175b0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAttElEQVR4nO2de5RdRb3nvz9CIAooYoJgRAOIzBUdhIkRxseFcVRA7kTv9QHXpcjozcUFd64j4xjRketlIcgVEQgQUJCnqEhAJCEQQgKSkEfn2Z13J+TReXbSnU46nXSnu2v+OLtP9jlnv2rvql1Ve/8+a2Xl9Dl717u+VfXbv11FQggwDMMw7nOU6QQwDMMwamBBZxiGKQgs6AzDMAWBBZ1hGKYgsKAzDMMUhKNNRTxy5EgxZswYU9EzDMM4yaJFi3YLIUYF/WZM0MeMGYOmpiZT0TMMwzgJEW0K+41NLgzDMAWBBZ1hGKYgsKAzDMMUBBZ0hmGYgsCCzjAMUxBY0BmGYQoCCzrDMExBKIWgCyHw9KI2HDo8YDopDMMw2iiFoL/euhvXP7UMN09dZTopDMMw2iiFoHcf6gcAtO/vNZwSpp717d34zuOL0Nc/qC2Oua27MWbiVGzde1BbHAxjA6UQ9DKxo+sQunoOm05GYn44pRkvtOzA4s2d2uJ4cuEWAEDTxo5U969v70b/gL4BRzez1+zCyyt3agtfCIHtXcUcLAcHBbY5NBEolaALFP+4vQtumYlP3vaK6WQUhrbOHnz69ldxywurY6+95rFF+NXLa3NIVYXBQYHbX1qDXfsPRV73zd8uxLcf1bdv0mPzNuHCW17Bim1dodfs2n8InQf6Usdx6PAAug7mP1GZNKsV//XWV7Bpz4Hc405DKQSdyHQK8mWfZ2IqErPW7MJn73gVh3OeKe/projQwgSz++krduBXL69TFvc9s1px3e8Wh/6+cGMH7n6lFd9/armyONMwb8MeAMDG3T2Bv/9h4WaMu3kmzrtpRuo4/v7euTj3py9h7c79ubSBwwODuPPldZjhrWy2d0UPmrZQCkH307K1C2t37jedDO1MWdyGmav0LbOjEEKgZWv4bC2Mnr5+7AjpODdMacband2leg7yHy+uwfPLt4f+PuAd8N7bb7f31g+ebs4cxsrt+wAAn73jNdz0/MrM4cXxp0VtuOPltWiua8d/WLgZzW3ybTsvYgWdiE4jollEtIqIVhDRvwZcQ0R0FxG1EtFyIjpfT3JrmdO6G0/MD91JMpDL734dn73jNU0psofv/XEZvvWIme2Jn2pqw+V3vy49oFzxwDxccMtMTanKhlBorVu8uVN6ljk4KDKZLIpE00b55y0dB/qwaFPy+3pDXJx/8HQz/m7S69Lx79x3CE81bZG+T5YkM/R+ANcLIf4GwAUAriWiD9ZdcymAs7x/EwDcpzSVIXztN/Pxo2da8ogqkEOHB/DSih3G4q9HhRdHx4E+9PRlM9ms3lFZAb25u2J3PNDbjxe9cuo6eBiPzN0IEaCQyy2c+QSZ62at2YU31u9JFd7qHfvw9/fOxa0JbPJ+fjljLc67aQZ2dzeuUFQONkHs7enDV+5/w7qHg0IIDA4my/yXJ8/FP9w3V3OKwrnqoQX4/p+Wo0PzoBwr6EKI7UKIxd7n/QBWARhdd9l4AI+KCvMAnEhEpypPrWX8bNoqTHhsERZtOmJfnd6yHWMmTsXenvxnUx+/NfvD0PNvmoHP3yU/A4li4pRm/PNji7B2537cMKUZNz63Ak11syXdouSn80BfKpPQEFf/diGu/PW8VPcO2eRXeSaEpAwNiH5BIMg9HNq4O92DvWeXbMWCNztw/6vrU92vmqGmMvHpZpxxw7RE96xvN/tQc2ggHkg4AKVFyoZORGMAnAdgft1PowH41xNtaBR9ENEEImoioqb29nbJpNrHlo7KQyD/0/df//VNAEDrrm4jaUpCy9YuzF2/O/T3N1N2/DA2e+V0oLcfnd5Ap9PvPI4v3DsHl9+tdtDKSlfPYbR1Bj9UVMVFv5id6r5Js+wQ8nr+kIMJwzUSCzoRHQ/gaQDfFULUTy+CpgoNQ5EQ4gEhxFghxNhRowKPxNNK1CxwyeZOjJk4VXunsoHL734d//jr+jFZHWm8ivL0RNq0x746vvj22fjEz2flFt/WvQdxzWOLEm2HEWTmYewkkaAT0XBUxPwJIcSUgEvaAJzm+/s9ALZlT54q4tXiyQWbAVQetMqSp7mg7BS1qJPaVlW1tZunrsT0FTswc9WuwN8HBgUen7ep5uFtUcu+SMQeEk1EBOBBAKuEEL8Muew5ANcR0e8BfAxAlxAi3N+qIFDEtDLvxh/3cgnjNnm/S/Hkgs348bMt6O4t3jsNRSZW0AF8HMDXATQT0VLvuxsAvBcAhBCTAUwDcBmAVgA9AK5WnlJHMPUO06d/8Wou8SzdshdLN3fimx8/PZf46inZO2LGGHouZOLtzCKj+231WEEXQryOmH4kKj5o16pKlElcNZ/sz2km9YV75gBAakF3tXxt3jYiS8pszhcjT+neFA1D1gWMUYutQm9zu8hihrE5X3EEvcPgCrrLnQWd0UZQvyvbvjoMkycs6Ixy0mg2C30y/CYShyeqjCZKJei62j93rPwoa1HbYCLhdm4/pRB0XbO/qGC58TO64VUNU08pBF2GrDrMnUwdv5u/ueH0paIULw/45US3VxELukcWIea+GYOQt/u2bO3CDc804/qnlupJkyRWC3CGtFmdL0YaFnTGCuqFpdfbvGuP4T3AtZnrFISbKYiiLHUcg90WHYD7RjDV5aVPvdgkxTD6YEFnlGP7bosu41/JlNVaYmI7AldMU6USdF2VEhSsy2+z2UxZS9WGAc+WbQJcObDZBKUQdF19IaiT2eAvbBoey/KBW1p+2DCgJqEUgi6DEJUTe2T2Rf+nR5sw6ZV1GlPFDOFIv4pFdtDLc3VZZEyvnNltURO9/QM44Nuh0D8CX/yL2fjab+RO9HlifuWADFuWpSZpmM2kcFu0DZvTnabNFWVgZGopraB/8Z65OOfGF00ng/GwWTB1ILuEDzTvuWIHYKqw26ImVtaduq5KUNiGfoRqmbLbonWUbPwsDaUVdEYfaWaOLPTJqHFbzHlZU4RVVNo8uJL3kgl6eK2kExR7zhQtC1yuauDxs5iUQtBztTVyT1E0c2TpjoNt6PnhSlGXQtBlYC8Vu3GkX8ViyxLetBtf3pjOLbstKiBZo1UkFaZbjAU0zBxTuS3aJd02V2saTebZfTEphaAz9mPrRNHq3RZZk52D3RYVwDZ0M1Q1OsJtkYvLDGUztQxR9HyXQtCZfJER6WJ3L/WYPCQ6LroiP39yZRwolaCrrpSoib8rDcAWki6iylqsqleZbEMvJqUQ9Dybrs3dZEtHD8ZMnKo9nrKKbt7YpslFfkvatrIOoxSCLkORZ9Zz1yffQVIXWcvXkX4Vi/xuiwVumDliuhTZbVEBiZwW2WtRGQ1FGSBGrgmzzYKaJmWulT+TjFIIehIs7q+FpL64bS1/XWYE+d0WG29gUXYPdltUANvQK+Rt40yy26LN5cUUD1snDqoohaAnwZWHHk4gUZYF71/KMXlIdJwYstuieUol6KrrJEq3ity4ddA4oAaXX1lLlecbTBJKIei5vijKPa+8qpsztjU1dls0TykEXYasWuTK0ixXFG7O5Ui/ikXabVFPMkrXXk2vnNltUQFJGm1RhMIKQgrT76nB5a2ONC6Vrsw4GTlKIehJUDVuWt1RDKfNLzzOuC1astti0OVWtzUmEHZbVAA3fDNUl5cOuy3aOtAw6Sh6fZZC0JNgu7C4hMwsJLx/FbznKSAPcaqtyegITdundeLKQBAr6ET0EBHtIqKWkN8vIqIuIlrq/fuJ+mSqQfXr25Ezf0cagC3YutuiLas7U+ngZuwWRye45mEAkwA8GnHNX4UQlytJkQbyPd/CEgUwiJqZWrHLUcXcwpbBZogit311ez0Z9nIRQrwGoENrKmwiY08r8rIzNancFoNxXTLSpl/Xkp/ba17k03JV2dAvJKJlRPQCEZ0TdhERTSCiJiJqam9vVxR1PIncFjOWtws2trzEMGym5rLboqr61dFMeLdFF8hHIFQI+mIA7xNCnAvgbgDPhl0ohHhACDFWCDF21KhRCqJWh6oOW+RlZ1bYbTF9uMHXp0+cpcVdeKx3WxRC7BNCdHufpwEYTkQjM6dMIbbZGstCEXZbtHWgYdJR9PrMLOhEdAp5a2kiGueFuSdruHmTRvT9oy0PGkeQKQtZt8WC98dYagUpfWkkrSL/dbzbov3EerkQ0ZMALgIwkojaANwIYDgACCEmA/gSgO8QUT+AgwCuEDYf75ITpS8ASeIGAVPjpS0DNbstMkmIFXQhxJUxv09Cxa3RepI0zqwN2BYBCCKvk97VDOcWF6QC5M8UDfo2exmpnHq58Pwo6Sqivq8Uxm2xCCRpaFkb41DH4LVJAArcFotSrLZIXl6Du6uoNzK45bZoNUlGxSLb//ImTCtUuC2akiFV7SNtKFH6y5MIF3DHbbFQZBUMnviE46TboiWHRKsOgx9zmcF6t0UXULNZVEwcLOThpHJbtENweOWWHBfKKrXJz/6sASiJoCchzcjJIh6MzmJxpF9pwy+aWUQmjQ09LD4XHoaWBRZ0Tdg4oufV7WSyHn4tRfyVH7aIlal0JGnHLszMy0KpBD1R4yz4ksw0obM8O3QzN+TPFG28QUWZlc9tMRm6vIDYbVEFCeom1ZuiEfeUTaAiCVAN2eIpynhpS7OwJR0q0PGAtz7M7P2Z3RbVoWlmHnVPmWfsYU1XxazHmNui4d0Wo2a/JW5qDsFui0ZIqzlD9/HMPBw5t0U7ZMqW3RYDw8hwrx2lmz+m3TXZbVEFMptFlbWla6DaeTLstmh6fOT2kJw8H47mLcyutINyCHoCVNvQbXzyn9fqQWU8DTN5dUE7iV9YspRFmioKa9MuPAwtCyzoiqhORrlxS81mrHdbtKQ6ze22WM5tM1zd66ZUgp5ot0U+U1QZQZ2C3RYrqNhtUUWRqWyteU5mXN3kjd0WFaCrmUU1YJ6p+wZHdlusYk2rsCYh2dFh32a3RYvRJQb+0ba+wss8U0+y22I8YfZaM6iqTS17BaXyuU2ZECYl7LaonDzEwOaZuWmzRprdFk2XprbVnZLdFk2XjjnSyqNpbxV2W1RIVF2Wt2voQ8Uh0aYnkqbj10aKBs9nitpPKQQ9Dxt6fYW70gB0oGYWEhxGiYsVQG3+c/HFThCFzavSslEKQc+TEq+Cq8jM1BJvlpQuKZmxpTptSUcQJmbmugczV81ZpRL0JI0gazMp88w8lARnijraf1Ijv9tiIypER6UwGtviV9vF6mG3RQVoG219wTbYhksmUH7COnaWM0WLMk7a0izYTBINuy1ajMwsxF/sA4MCnQf6IgKOijNxlLnhVie2zG3R+KZO4aRJWZEfYNoJuy0qJ8lM3V/sNz2/EufdNAPdvf0ScaRIWElw0m1RcQLUvpmZA6YrIITUbouGBzJ2W1RI1CwrSOynNm8HAPSECbrvFhtn5KapFonLbouaEmB64E8sLBKbgZkWS5240r9LIegmnlg7Uv9aUFPc7LYYRNQqR0t8Ca5xy5RXbEoh6Ey+qNltsZai7bYoPeMzPaWPwIzbot77+EzRgpC2ociaEkoFuy1WSZ1NBRucBQarIIwhjLktJui05tsXe7koQ9+bom5h+oALObdFEfEXkxXzAmc36t0W86EUgi4jBjIVV/sadvo4mXDqq8P13RZTE9EwVR9w7gKypgvz+WW3RSOYr/jiksZt0TxqhxClJo4cRjdHJqaJsbaZKYIFPQNFa+yqKcJui7pSoEKMswyCSeOPWoU2Xmu+tsLIWt72TjhqKYWgmxBe028WmkRNebPbYhDBe7pojC/JA0cDPazE3SuSUgi6DFpOlCkZUWWY9lmDueLV5L6mUJAyhaUgHTbPzOtJOtHS9u6K5qIqlaCrHtWjKt3V7Te1kKrg9YuEiVVU1lbhvz9LE0tzq61NOtnLT6Zht0VlyDREqVMvI/yr2eQS8H2E22Koq6P3f3lLsxZV5ZBqiI25qUhvjLLbosXkqa08M9eLsdI1PKJE77aY5elozM8Fac/mJwTstqicJG3TfMUXlzRui0WrD+vezCxaAcdQ9OzGCjoRPUREu4ioJeR3IqK7iKiViJYT0fnqk6mGqJl6molIUWYvuqjOHFO5LdpStha7LWZIW9Loawfh6PjyfDgqu+rOWtyuWFCTzNAfBnBJxO+XAjjL+zcBwH3Zk6WWXF7AsEV/IshtANIYjyP9ShtBwqLTdp1EyIpkO3edWEEXQrwGoCPikvEAHhUV5gE4kYhOVZXAvAl6mCkAPL98G/r6B/NPkItEqEDqzc9SJsVWlLotZhjmXHI5VIH53Rb1osKGPhrAFt/fbd53DRDRBCJqIqKm9vZ2BVHnw+w1u3Dd75bgjpfX1nzvr3JXlmRGiNgpMEL6NSXGF4OBOsvstug/xzZDaCr1ysj2ub44E60ijC+h3XFbDEppYBELIR4QQowVQowdNWqUgqjlSNLwgiq+s+cwAGBH16G68KLiKjEhnSfabz/ke+//UpenD1WDkI7BrEimF1fdjlUIehuA03x/vwfANgXhOklxmrSdlHW3xagJZqY9XWJK1PjEVjFpVxOulIMKQX8OwDc8b5cLAHQJIbYrCFc5SWYQro7MLsBui/bttlg2G7o58inno+MuIKInAVwEYCQRtQG4EcBwABBCTAYwDcBlAFoB9AC4WldisxLVeNMsF1W9hp0XeScx226LdhSofYdEy9mOs8ZfE4dFuy3mPe9yZZ4XK+hCiCtjfhcArlWWIg0UybbnAjpL21S/Mt2CouLXuttigmu4f9lDrKAzjaNzW2cPHpu3SWqv6DLRUBQJzhSNC8W0ZKiuXtn2IrODpc50OI9pt0XN5c2CnoJrn1iMZW1deP/Jx5tOilMEbc4V176LNtvPnp9w85VcKOpK1ozbolz8picELrktOkPql1rq6uLwgPDCiwiwbDMfH0XamtU+1DQsHSJcJNOLq84RpRJ0VQyJT96nx5QRUfe/KUx38MjdFjUmrWjNOW1RudKvSyXoUZWSxs/XOS8Xw2mUc1t0oEBTYHpgGsL12bTpAVYe3j5XOarbQPSboq41uPTs2ncInQf6Gr6vlkCGQ6JNo6sW0w6uubvrhXwOvra4bd6V8aMUD0WzzkwbRCggPFcqXAfjfjYTALDx1s8rDDW4QE3NzEzvBZJm2wQV8G6LblGqGXpa0mgIN3Lf7DuV22JtGKZLU/VAIu22qGEHS8AeE5AsaV2Gze+2qLfEWdAZbdQ33TRuizpx8pBo0yNbAC6YWsyXG7stKkdV/63OviO9Fu1r5HmtGtKd/qQ+HUVE9RgUV+xyB6xzJZqmVIKehKy+6tyk1WKN26Lh+KNMAFnSZjpfaUnbT5NOtOpXb65MOEol6JFui/klo7Sw26I9AuqKQBUHdltUjjKTC3u5JGOoUBS6LRZll73UbotqkxEfn/B/jo7dRjOjKlzp36UQdJ6M2IOrm3Opjl/lqfV697tJsk+KgdpJa3IxLMy64y+FoKuiodmaVhnbqe6RkL4V27I5l6p+mDY/0S+xpUemakz74mdBdtBRn1f2crGOxm1hTaQiPbn3x4YHS3a5LbqISUkNM7kU2dTiGqUSdFUNL0iM6sXS9NLOJKlOfwq5pcTFGIjq8ogb5GXqMk/TCw8iwZRK0JOQtqEcef7n7rLULmrL0XT3NT1AR++2mD5xpvOVN0mzy26LDhA1g3ClwlwmzSHRRcOWbKZp77ak3U3YbdEaGhq/94V7W3jmS7bdFu0oW11LexW7LebR/GTyz4dEm6dUgp7VnFJPmc0raQezJLf5w7Zlcy7VVa1SILK0w7g+UbTdFk1PwnTHXg5Bjzy8Ir6Ihxq9O802mLzTH7TbYsNvBtGxS18cOjbnSidSKR5cp4glDxJlXzLx7LZYYIZmIKL6d4WamaStrd0gUbstMqZJNgj4qyzsDvY4sYdSCbrO1VZ92K7Y3HQgo9vxp+AwftTvtqhukM3XbZEJolSCHkQSIZYxufAcNB2Ng4BlbouGU6BrcWM6X3mTdEBkt0UHiD4kOsKlsd7k4kjl2oac22IxhcaeXMk34jKvOrPDbouMYvIeiETACCi722L970VxV0vvtugfFPUXhtSD41zdFvNtCK4MZqUS9GBzSnJ4Yn4EqQYueaZo0DWmy161fdgWgVCRDpfcFpOi7UxRzRVfCkHP2uDqZx7Fa756aJiNB2zOZRITL81kd1s0V3K2mhqTbfFrGnZbtI4Gu6/EtWVExWSEy1EvUh5JIZVh4sEqt4tgSiXoQY0gzRIo8gGq+amAcWSKILz0uSCDMP2mYxQumF4sLj4llErQkxBV3/Y3V3eJGwhN90PTQhAlllnSZjpfeSE70WK3RQfIeoxXkh0C7e4gZlsluy3akyvXd1uU7Wfm+yW7LeZCkmKubwyODNbGqdpWVbot5iwr1rkt+j8bF6laivySkm1lHUapBD1rnSR6U7Qkai9VlrJuiwHf6ShWufM0zcUdR5a0xW69kCChpm3nOnzl9bktagm2SikEvSwiaxv1Hd02t0UZrNltMeC7vGaPNtWZ7GrAvAaw22Iu6OoMNnsj5IWKJTiXYi2qyyPW5OVTQt5t0X5KL+j1ROnwlCVbMWbiVPT2D0aEYHwqYBw1uy1yOeaBSik2bXpJQtHnWYkEnYguIaI1RNRKRBMDfr+IiLqIaKn37yfqk6oZibbY3dsPwL2ZifllZziNaRMRf+WPcSGIOqQlRelY3BSSIZll2cHG1RX20XEXENEwAPcA+AyANgALieg5IcTKukv/KoS4XEMataJelN1sCHmQ5pBo54WnDpdbh6siB2Tv59knQ/a4LY4D0CqE2CCE6APwewDj9SbLABHlHdaQXVhi5smG9u6av4N2W6wn+CFfeGXkvtuizYdEq0lKTHz573djI0K4MaAlEfTRALb4/m7zvqvnQiJaRkQvENE5SlKnGsWeCsENuBwiH9S4r/rtgrCLM8dnulStdlvMUjoxCfH/GubKZ2JiI0I+y9wXhavHJSYR9MBJVN3fiwG8TwhxLoC7ATwbGBDRBCJqIqKm9vZ2qYTq4sgsUuImN+u6gdumr1Ya3sBA9OvSadwWbZkTWeO2GHRItMZS8kfnwgw1DPOraXvcFtsAnOb7+z0AtvkvEELsE0J0e5+nARhORCPrAxJCPCCEGCuEGDtq1KgMybYfF5r+vbPXo38gymNHjqR28WSY7oB2olxTFcxEi2xqcY0kgr4QwFlEdDoRHQPgCgDP+S8golPIm34R0Tgv3D2qE5sHQY0zdNvQoIMYLNahoKTpWFrKhBlhLVeRlMKgrVkpHCHMz4LjcXmVkYRYLxchRD8RXQfgRQDDADwkhFhBRNd4v08G8CUA3yGifgAHAVwhLCq5JCmRaYz2N1szZKnxsDHgyPMKJow05W7zxCMJsnnOutuiK8QKOlA1o0yr+26y7/MkAJPUJs1NbG4HQTPnSsPNp3encVssGi5n0+m0Z0x8kdwWS0EZ7IBBsw6VuQ4tQ1m3xcg48sW2gcZfxnnMIuU8SCwrrADSprDitqg0KVoolaCnbXD1d/GJRcEdI7TBB/wgW0ymd1tUhY586Gxz/jIKi8aM26LcwCZbRrzboqMMFbCuE2FsItjkYjb+OApS9FrRWYf+KuO6yII9botMCooyCMjQYBc3koqCY2GhumBqKQss6HXIuCIGXqs2OdqxvTO6Vp660GVWsbv2w0k7YSr6RKtUgh54BqjXpKM6jMwRdDa3F92HI6QJy+by8mOzG1ualOmwe9vshy6bMpvrO4pSCHraWai9zTMdpn0iotwWs4XsDk7nw+HEZ006uy1aiPTLBWniSHGPi0hNYKR3W4yK14yq2LJZU9pNqULDiwlEx3mdKkgfU0pPN6GovDWXUakEPdDkUqK37HQnW4fbYpq905Ji4vmBFrdFDWHKRGTa1JKkFqXdZJV3cvZysQYdHV8IgQdffxN7e/qUhy2Xjvzikuskdo2aNttUs6RMqkrsLQLGgwU9guo+IqkacvRNizd34qbnV+IHTy9PE7gyVA5Wal/nZ/UIwuJxJVdsHmBNUnpBT/KaecO+3p7UBzWqpDOeoYOmuw4eTnaDAwyVh8rVql3zdHPoMmuo0EXbXV/9FH0cKJWgR9VlpCuizIMhxxqMUrfFnO4xgc3pTDNb1fEcyLQtPQpZm7irK4ByCHqCuinaJkRJyTMnLrotqu7XLrccm9q9bL1kFWh2W7QQ2QOJ09ShzR4wKtOmqnMH77YYHrbS3SElArOlWms3pVIQXuyZoskjMSX4cq6V6eNQMWvnzblyImob17A6sMU3OQsql5ZxQWU5U9T9kq6gZ7fFfEonzKRis6llCNkyctVtMdEBF0VB9eDoF8Md+w4pDl09qtvod3+/BAs3dqoN1FLsMTY0kmVQljsu0OZSYICSCXoQqR7kJbHJx11joG8E72WTnmeX1pwV3iAsKgTA/rlfvjj6rM4ail5+bHKpwy9KqWzokneZXq7q8HIxnaciom23RRV2YQdm7mVpkaUS9MgHoAE1Xr065L6gb2Ubd56dQbuptdC7LZpOQTh57TkU+4wkR9nUXR/stmgxkS8Pidr/S4fOfNeFncVt8UgY6ZOTKj7FBWRzaDUhpyhom2fqWVPGbosWourJdZjnhRAi+SzF8TWgKmHVvUd7FDLR2OLQFJRm00kzbmKTcls0vduiXkol6FEEddjqXi4h96jdu0Q/QR1P514u9dGlcVssGq7ke6iuag6JdiXxAfBuiwUkcBkZoWeuvM2YlCDxVntike0lkB6bs5ZX0mwqA5vNOyYplaCHMWvNLtwzaz2AbPu2qLARu0xDnstYCJpRPWiWrYpsGpR04KQfendvP1p3dSsL7+6Z65SEc9uLa5Jf7GtYL6/ciVPePgIfGv12JekII9jkopavTH4DCzZ2KA6VUW0BUGlScGK27LC5SAYnBf07jy/CX9ftlr4vzM3wqJDGLbsf+gOvbcAl55xSc28cBMK3H20CAGy89fMJ77ITISAt5g5IAQDLRSuvh8glclt0FSdNLku37JW6Pn6PkeDPWdqMK+1Np9278aGxC34CdbEpjs7U5mKp45C61uJWL5m0+rZKlLW82W0xlLTzgLD7/MvPZK/1y72glJSd+w5hedve9AGkwHQXjCsurW7yEj3UuGueR2CKc0paWNs2XTZSO0KmbFCqBk/djgNuCrpK+58AjooJrr7ByFTJ4/M2JRbpi38xG/9j0hyJ0OWIa/grtnUpDb++WIuwO2VWuATUIC2LkgXPbos5Ui/A7ft7E90X1gjibOhh30fu2+399ONnWxKLdE/fQKLrVDKUzkWbOvH5u15XEpZKbBHAIpkTUkdjcREwFZwU9PrR86M3v5wxvCOf9xzoq34ear/b9tZujRvVrmUHdtNCMRR/W2ePgrCi/65nYFBg/psdNelgYlBt0y9ZsRe9nTkp6B0+0R3iuWWVrVwXbuzA9JbticMSqJ2hPzx3Y8M1zyzZmjw8hba2wcFiN77e/iMrkrtntoZet3BjBw4YWL3YhGk7dRQmRFLWFm1v6anFSUEP4n89uQQA8OXJb+CaxxdL3ZvWXhbVpuqD7O7trzbCr97/Bv7xN/MBBA9OADD+njk444ZpqdJVz+Y9PZi1elfwjwbHDH/5vbFhT+A1HQf68OXJb6Dr4OGcUhVMWF0LIfDffjEbUxa35Zsgfxpyiyc6JpsHHVkO9PWbTkIqCiPoADCntdY3vfNAH9o6e7B1b8WcsGTz3oZ7DvT2x9rKZXihZQeARgH40I0v4t7ZlbdRh8wMALB2Z/ALUsvbsj2g9HPx7bNx9cMLtS+v+/oHa/5uOPCi4QCMYPyXHTw8EPpbHsRF1z8osGH3AXzvj8tySk++byPLnddp74py36GKQCfNz23TG18SzJa/fMrGyReLwnjw9Ternw8dHsB5N82IvefW6atD7d6hVZCgblq2deGyD59S891zS7fh2ovfH39zAJv39GDjngP41AdGSd338+mrMRBhuknbzFQIa1S6VMQzMCgw6ZXWmtn9Id8AIRN0WBtJkgfd5DUvtulM0Uff2FT9HNdGFm1K/rLbq2vb8b6T3po2WbHobi2FEnR/s5r0SrhN1s++g4cxfJjkQsWLKKpy7pu9HqOOP7bmu8ODgyFXhzNvwx78eek2PL24DX39g1hww6dx8ttGJL7/Pm9VEIbOGW+9Kav+b93PCGat3oU7Xl5b890PpzQrjcMGQc/K/A178KHRb8dxx7ojB0HPuoY4PDCIQSFw7NHDAABtnQcTh3vVQwuyJi0EdlsMJOphyEyfnXjSrGSC3j8gcHggWGhD3SHFUFqiw/6Pur1dNrQfwB0z1oZcfYQXV+yofr7igXl4csHmqjlj3M9mYmBQYPHmTiza1IEbnmnG1x+cHxtmGAICd768Dj/584rUYcSxLMR8NJBiNFnffgAA8Mb6PVgYs81AX129Ltm8V+oBt5+gpL60Ygf6Ewj6nNbdmPBoE/oHBvHE/M3V79e3d2PBm8F5OCjxEFimFJ9csLmmD+3u7sVXH5iH//2HpQniqdy3paMn1itq9Y59GDNxKtbs2C+ROjX891++irN/PL36t98UeJeifZtsxZ0h2eMvy5N7sCQh7GFcEnZ3R/u/19t/AeDOBA3qnx9bVP38luHDGsK5d1Yrbk8wMNRz87RVDd9NeqW1RmSG2LX/EE4+YQSaNnbgzFHH4y3HDKvZEC0u70Bl8I1a7g6GCLq/A+6qG1SveXwR3vW2Y7FzX+X7p665EB8dc1L19+eXb8N1v1uCZTd+tkGE62d1/QMC01u2o29A4L0nvRUjhh+FdTu7MTAo8IXzRtdc293bj2Vb9uLc006sfjfhsUW4ctx7Q/M3xNe8B+Bz1u/B1OYj7ffTt78KIHgPnx89G7ySCCqy7t5+HDo8gBHDh8Wm5YdTmnH6yOOqfw+V9Surd+G3c97EKW8bgUs/fGpkGJ+8bRYAYPVNl2DE8GE1eQKAG55pxsxVOwGg4Td/vMccXTuffKF5O/7TqW+rSd9LK3Zg9DveEpsvAFi2ZS8+PPrt2LSndrD5/p+WHwlv5U5c8qvXcOPfnYMLz3wnAOC26avx4oodmHn9RYnisZlEgk5ElwC4E8AwAL8RQtxa9zt5v18GoAfAN4UQcq4mCTlmmPkn6d29+T0BDxoU0og5UFkh1BMk5gAw7uaZuOjsUZi9ph1HH0UNM9GhTh3FX9ftxr6DtWW1ZEsnAGDVjv04fdRxQbfh3H9/KTLcITEHKl5NXxn7Hoz/yOiqcALAuT+NDgMA7n9tPe5/dUPgb+M/8m589f55VdHp6RvA+HvmYPVNl9Rc9+SCxvIbGBSYsXInhg8jfOuRpur3f1y4pfp5e9cRM4Df9HTmDdOw9CefqZnZ3jFjLfYerHhDfeuRJrT89HMYcfRRVXNP+/5efPr2VzH7+xfhS/fNra6Izhh1XMODaqAi3kPPFX7jPXfqHxT46V9WAqgdYPz5e3HFTpzzkyMz3//58EL8+/gPVf+ev6EDl5xzCn7na1N+11QAGDNxKh6++qP45m8X4sGrxuL9Jx+PU94+Aut2duM7TxyRjCe+/TF8/P0jMcE3uQnje39cillr2hu+//GzzTWD/RCrd+zHlb+eh3e8dTgmfOrMqrPCLS80TniGeGX1Tpx58pH2OmPlTrz7xBFYtX0/vvCRd+OHU5rRsm0fLv/Pp6LjQB/mtO7G9O9+qiGclq1d2NzRg4vPPjk2X2mgOH9OIhoGYC2AzwBoA7AQwJVCiJW+ay4D8C+oCPrHANwphPhYVLhjx44VTU1NUZcEMr1lB655PL6SGYZJz0nHHRPqUpsXM6//2+oqxkWOIiDMItd686U4WvbZnQcRLRJCjA2MM8H94wC0CiE2CCH6APwewPi6a8YDeFRUmAfgRCKKXrelZOyYd+gIlmEYH6bFHIDTYg6EizkA/D9Nz6ySCPpoAFt8f7d538leAyKaQERNRNTU3t64RErCyOOPxV+u+wS+eN5oXP+ZDzT8ftbJx9f8PWJ4tue+x/rsfMPrzD2XffgU3PzFD+HEtw7HxWcfcSf8p0+ejg+8q5KOs991QmjYJ4R4FYw6odY75rSTktkQg3jnccekvjeK73/ubC3hylJvh03LWxLYnxlGFZ88a6SWcJOYXL4M4HNCiG97f38dwDghxL/4rpkK4BYhxOve3zMB/F8hRKhtJK3JhWEYpsxkNbm0ATjN9/d7AGxLcQ3DMAyjkSSCvhDAWUR0OhEdA+AKAM/VXfMcgG9QhQsAdAkh1PoXMgzDMJHEui0KIfqJ6DoAL6LitviQEGIFEV3j/T4ZwDRUPFxaUXFbvFpfkhmGYZggEvmhCyGmoSLa/u8m+z4LANeqTRrDMAwjg3Ov/jMMwzDBsKAzDMMUBBZ0hmGYgsCCzjAMUxBiXyzSFjFRO4BNsRcGMxLA7tir3IPz5RacL7coSr7eJ4QIPOnGmKBngYiawt6UchnOl1twvtyiqPnywyYXhmGYgsCCzjAMUxBcFfQHTCdAE5wvt+B8uUVR81XFSRs6wzAM04irM3SGYRimDhZ0hmGYguCcoBPRJUS0hohaiWii6fTIQkQbiaiZiJYSUZP33UlENIOI1nn/v8N3/Q+9vK4hos+ZS3ktRPQQEe0iohbfd9L5IKL/4pVHKxHd5R04boyQfP0bEW316mypd4bu0G/W54uITiOiWUS0iohWENG/et87XV8R+XK6vjIhhHDmHyrb964HcAaAYwAsA/BB0+mSzMNGACPrvrsNwETv80QAP/c+f9DL47EATvfyPsx0Hry0fQrA+QBasuQDwAIAFwIgAC8AuNTCfP0bgP8TcK0T+QJwKoDzvc8noHLo+wddr6+IfDldX1n+uTZDT3JgtYuMB/CI9/kRAF/wff97IUSvEOJNVPabH5d/8hoRQrwGoKPua6l8eAeJv00I8Yao9KpHffcYISRfYTiRLyHEdiHEYu/zfgCrUDnz1+n6ishXGE7kKwuuCXqiw6gtRwB4iYgWEdEE77t3Ce+EJ+//k73vXcuvbD5Ge5/rv7eR64houWeSGTJNOJcvIhoD4DwA81Gg+qrLF1CQ+pLFNUEPsmu55nf5cSHE+QAuBXAtEX0q4toi5BcIz4cr+bsPwJkAPgJgO4Dbve+dyhcRHQ/gaQDfFULsi7o04DuX8lWI+kqDa4Lu/GHUQoht3v+7ADyDigllp7fsg/f/Lu9y1/Irm48273P991YhhNgphBgQQgwC+DWOmL2cyRcRDUdF9J4QQkzxvna+voLyVYT6Sotrgp7kwGprIaLjiOiEoc8APgugBZU8XOVddhWAP3ufnwNwBREdS0SnAzgLlYc3tiKVD2+Zv5+ILvC8Cr7hu8cahkTP44uo1BngSL68NDwIYJUQ4pe+n5yur7B8uV5fmTD9VFb2HyqHUa9F5Qn1j0ynRzLtZ6DylH0ZgBVD6QfwTgAzAazz/j/Jd8+PvLyugUVP3gE8icpy9jAqM5xvpckHgLGodLj1ACbBe3vZsnw9BqAZwHJUROFUl/IF4BOomBCWA1jq/bvM9fqKyJfT9ZXlH7/6zzAMUxBcM7kwDMMwIbCgMwzDFAQWdIZhmILAgs4wDFMQWNAZhmEKAgs6wzBMQWBBZxiGKQj/H58vu86K6sIGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_black_simult = [tensor.item() for tensor in losses_black_simult]\n",
    "plt.plot(losses_black_simult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1575206ff10>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_black = [tensor.item() for tensor in losses_black]\n",
    "plt.plot(losses_black)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:<br>\n",
    "https://towardsdatascience.com/deep-q-learning-tutorial-mindqn-2a4c855abffc<br>\n",
    "https://towardsdatascience.com/all-ways-to-initialize-your-neural-network-16a585574b52<br>\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html<br>\n",
    "https://huggingface.co/learn/deep-rl-course/unit3/deep-q-algorithm<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
